{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "nLSM-lVXZnRS"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow_gnn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb # use pdb.set_trace() for debuging\n",
        "import tensorflow     as tf\n",
        "from   tensorflow import keras\n",
        "from   keras      import layers\n",
        "import tensorflow_gnn as tfgnn\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "from   plotly.subplots  import make_subplots\n",
        "import plotly.io            as pio\n",
        "\n",
        "import numpy  as np\n",
        "import pandas as pd\n",
        "\n",
        "from GraphFun.generate_graph_Econn      import generate_graph_Econn\n",
        "from GraphFun.plotGraph                 import plotGraph\n",
        "from GraphFun.make_edge_bidirectional   import make_edge_bidirectional\n",
        "from NormalisingFun.normalise_dataframe import normalise_dataframe\n",
        "\n",
        "from EdgesetFun.edgeset_function_1_0 import generate_graph_multi_sets\n",
        "from EdgesetFun.label_function_1_0   import label_function"
      ],
      "metadata": {
        "id": "OsIBi_TBZoCe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Edgeset Generation\n",
        "\n",
        "n_nodes_list_train  = [15,15]\n",
        "n_edges_list_train  = [(1, 2),(1,2)]\n",
        "n_conn_list_train   = [[1,1],[2,2],[1,2]]\n",
        "n_edges_ij_train    = [2,2]\n",
        "\n",
        "n_nodes_list_test  = [15,15]\n",
        "n_edges_list_test  = [(1, 2),(1,2)]\n",
        "n_conn_list_test   = [[1,1],[2,2],[1,2]]\n",
        "n_edges_ij_test    = [2,2]\n",
        "\n",
        "n_nodes_list_val  = [15,15]\n",
        "n_edges_list_val  = [(1, 2),(1,2)]\n",
        "n_conn_list_val   = [[1,1],[2,2],[1,2]]\n",
        "n_edges_ij_val    = [2,2]\n",
        "\n",
        "bidirectional = True\n",
        "\n",
        "df_list_train, edge_df_train = generate_graph_multi_sets(n_nodes_list_train, n_edges_list_train, n_conn_list_train, n_edges_ij_train, bidirectional)\n",
        "df_list_test, edge_df_test   = generate_graph_multi_sets(n_nodes_list_test, n_edges_list_test, n_conn_list_test, n_edges_ij_test, bidirectional)\n",
        "df_list_val, edge_df_val     = generate_graph_multi_sets(n_nodes_list_val, n_edges_list_val, n_conn_list_val, n_edges_ij_val, bidirectional)\n",
        "\n",
        "# Nodeset Generation\n",
        "node_df_train = label_function(n_nodes_list_train, df_list_train)\n",
        "node_df_test  = label_function(n_nodes_list_test, df_list_test)\n",
        "node_df_val   = label_function(n_nodes_list_val, df_list_val)"
      ],
      "metadata": {
        "id": "_OiK8ujncFbn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def edge_features(edge_df):\n",
        "  features_len = len(edge_df)\n",
        "  features_array = 1 + 0.5*np.random.uniform(-1, 1, size=[features_len,1])\n",
        "  features_df = pd.DataFrame(features_array, columns=['edge_feature'])\n",
        "  edge_df = pd.concat([features_df, edge_df], axis = 1)\n",
        "  edge_df = edge_df[['edge_feature','source','target','set','source_nodeset','target_nodeset']]\n",
        "  return edge_df\n",
        "\n",
        "\n",
        "edge_df_train = edge_features(edge_df_train)\n",
        "edge_df_test = edge_features(edge_df_test)\n",
        "edge_df_val = edge_features(edge_df_val)"
      ],
      "metadata": {
        "id": "XGD4A_bGmqNl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NormTYPE = 'STD'\n",
        "node_df_train,  node_df_train_mean, node_df_train_std   = normalise_dataframe(node_df_train, NormTYPE)\n",
        "edge_df_train,  edge_df_train_mean, edge_df_train_std   = normalise_dataframe(edge_df_train, NormTYPE)\n",
        "\n",
        "# different from conventional database, here the normalisation is based on the training and\n",
        "# applied equally to the other graphs to avoid diffferent normalisations\n",
        "for iloc in range(len(node_df_val.keys())):\n",
        "    my_key = node_df_train.keys()[iloc]\n",
        "    if node_df_val[my_key].dtype != 'bool' and my_key!='set' and my_key!='ids':\n",
        "        node_df_val.iloc[:,iloc]  = (node_df_val.iloc[:,iloc]-node_df_train_mean[iloc])/node_df_train_std[iloc]\n",
        "        node_df_test.iloc[:,iloc] = (node_df_test.iloc[:,iloc]-node_df_train_mean[iloc])/node_df_train_std[iloc]\n",
        "\n",
        "for iloc in range(len(edge_df_train.keys())):\n",
        "    my_key = edge_df_train.keys()[iloc]\n",
        "    if edge_df_train[my_key].dtype != 'bool' and my_key!='source' and my_key!='target' and my_key!='set' and my_key!='source_nodeset' and my_key!='target_nodeset':\n",
        "        edge_df_val.iloc[:,iloc]  = (edge_df_val.iloc[:,iloc]-edge_df_train_mean[iloc])/edge_df_train_std[iloc]\n",
        "        edge_df_test.iloc[:,iloc] = (edge_df_test.iloc[:,iloc]-edge_df_train_mean[iloc])/edge_df_train_std[iloc]\n",
        "\n",
        "# edge_df_train = make_edge_bidirectional(edge_df_train)\n",
        "# edge_df_val   = make_edge_bidirectional(edge_df_val)\n",
        "# edge_df_test  = make_edge_bidirectional(edge_df_test)\n",
        "\n",
        "if len(node_df_train)<=100:\n",
        "    plotGraph(node_df_train, edge_df_train)\n",
        "\n",
        "\n",
        "print('---------------------')\n",
        "print(np.mean(node_df_train['node_feature'].values))\n",
        "print(np.std(node_df_train['node_feature'].values))\n",
        "\n",
        "print(np.mean(node_df_val['node_feature'].values))\n",
        "print(np.std(node_df_val['node_feature'].values))\n",
        "\n",
        "print(np.mean(node_df_test['node_feature'].values))\n",
        "print(np.std(node_df_test['node_feature'].values))\n",
        "\n",
        "print('---------------------')\n",
        "print(np.mean(edge_df_train['edge_feature'].values))\n",
        "print(np.std(edge_df_train['edge_feature'].values))\n",
        "\n",
        "print(np.mean(edge_df_val['edge_feature'].values))\n",
        "print(np.std(edge_df_val['edge_feature'].values))\n",
        "\n",
        "print(np.mean(edge_df_test['edge_feature'].values))\n",
        "print(np.std(edge_df_test['edge_feature'].values))\n"
      ],
      "metadata": {
        "id": "w_57VAzY257O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Issue with the function below, I think. I want a way to inspect the tensors created by this function."
      ],
      "metadata": {
        "id": "9AVcY_-nDZog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_graph_tensor(node_df, edge_df):\n",
        "\n",
        "    id_ns1  = node_df['set']==1\n",
        "    id_ns2  = node_df['set']==2\n",
        "    id_es1  = edge_df['set']==1\n",
        "    id_es2  = edge_df['set']==2\n",
        "    id_es12 = edge_df['set']==12\n",
        "\n",
        "    # pdb.set_trace()\n",
        "\n",
        "    graph_tensor = tfgnn.GraphTensor.from_pieces(\n",
        "        node_sets = {\n",
        "            \"node_set1\": tfgnn.NodeSet.from_fields(\n",
        "                sizes = [sum(id_ns1)],\n",
        "                features ={\n",
        "                            'node_feature': np.array(node_df['node_feature'][id_ns1], dtype='float32').reshape(sum(id_ns1),1), # input feature\n",
        "                            'label':        np.array(node_df['label'][id_ns1], dtype='float32').reshape(sum(id_ns1),1),        # label\n",
        "                          }),\n",
        "\n",
        "            \"node_set2\": tfgnn.NodeSet.from_fields(\n",
        "                sizes = [sum(id_ns2)],\n",
        "                features ={\n",
        "                            'node_feature': np.array(node_df['node_feature'][id_ns2], dtype='float32').reshape(sum(id_ns2),1), # input feature\n",
        "                            'label':        np.array(node_df['label'][id_ns2], dtype='float32').reshape(sum(id_ns2),1),        # label\n",
        "                          })},\n",
        "\n",
        "        edge_sets ={\n",
        "            \"edge_set1\": tfgnn.EdgeSet.from_fields(\n",
        "                sizes = [sum(id_es1)],\n",
        "                features = {\n",
        "                    'edge_feature': np.array(edge_df['edge_feature'][id_es1],  dtype='float32').reshape(sum(id_es1),1),        # inputs\n",
        "                    },\n",
        "                adjacency = tfgnn.Adjacency.from_indices(\n",
        "                    source = (\"node_set1\", np.array(edge_df['source'][id_es1], dtype='int32')),\n",
        "                    target = (\"node_set1\", np.array(edge_df['target'][id_es1], dtype='int32')))),\n",
        "\n",
        "            \"edge_set2\": tfgnn.EdgeSet.from_fields(\n",
        "                sizes = [sum(id_es2)],\n",
        "                features = {\n",
        "                    'edge_feature': np.array(edge_df['edge_feature'][id_es2],  dtype='float32').reshape(sum(id_es2),1),        # inputs\n",
        "                    },\n",
        "                adjacency = tfgnn.Adjacency.from_indices(\n",
        "                    source = (\"node_set2\", np.array(edge_df['source'][id_es2], dtype='int32')),\n",
        "                    target = (\"node_set2\", np.array(edge_df['target'][id_es2], dtype='int32'))\n",
        "                    )),\n",
        "\n",
        "            \"edge_set12\": tfgnn.EdgeSet.from_fields(\n",
        "                sizes = [sum(id_es12)],\n",
        "                features = {\n",
        "                    'edge_feature': np.array(edge_df['edge_feature'][id_es12],  dtype='float32').reshape(sum(id_es12),1),        # inputs\n",
        "                    },\n",
        "                adjacency = tfgnn.Adjacency.from_indices(\n",
        "                    source = (\"node_set1\", np.array(edge_df['source'][id_es12], dtype='int32')),\n",
        "                    target = (\"node_set2\", np.array(edge_df['target'][id_es12], dtype='int32'))\n",
        "                    ))\n",
        "      })\n",
        "    return graph_tensor\n",
        "\n",
        "train_tensor = create_graph_tensor(node_df_train, edge_df_train)\n",
        "val_tensor   = create_graph_tensor(node_df_val,   edge_df_val)\n",
        "test_tensor  = create_graph_tensor(node_df_test,  edge_df_test)"
      ],
      "metadata": {
        "id": "hE3V34ZG7A3Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def node_batch_merge(graph):\n",
        "    graph              = graph.merge_batch_to_components()\n",
        "    node_features_set1 = graph.node_sets['node_set1'].get_features_dict()  # returns a dict of tensors\n",
        "    edge_features_set1 = graph.edge_sets['edge_set1'].get_features_dict()  # returns a dict of tensors\n",
        "\n",
        "    node_features_set2 = graph.node_sets['node_set2'].get_features_dict()  # returns a dict of tensors\n",
        "    edge_features_set2 = graph.edge_sets['edge_set2'].get_features_dict()  # returns a dict of tensors\n",
        "\n",
        "    # label = node_features_set1.pop('label')\n",
        "    # _ = node_features_set2.pop('label')\n",
        "\n",
        "    # label = node_features_set2.pop('label')\n",
        "    # _ = node_features_set1.pop('label')\n",
        "\n",
        "    label_1 = node_features_set1.pop('label')\n",
        "    label_2 = node_features_set2.pop('label')\n",
        "    label = tf.concat([label_1, label_2], axis=0)\n",
        "\n",
        "    new_graph = graph.replace_features(\n",
        "        node_sets={'node_set1':node_features_set1,\n",
        "                   'node_set2':node_features_set2\n",
        "                   },\n",
        "        edge_sets={'edge_set1':edge_features_set1,\n",
        "                   'edge_set2':edge_features_set2}\n",
        "        )\n",
        "\n",
        "    return new_graph, label\n",
        "\n",
        "batch_size = 32\n",
        "def create_dataset(graph,function):\n",
        "    dataset = tf.data.Dataset.from_tensors(graph)   # 1\n",
        "    dataset = dataset.batch(batch_size)                     # 2\n",
        "    return dataset.map(function)                    # 3\n",
        "\n",
        "\n",
        "#Node Datasets\n",
        "train_node_dataset = create_dataset(train_tensor,node_batch_merge)\n",
        "val_node_dataset   = create_dataset(val_tensor,node_batch_merge)\n",
        "test_node_dataset  = create_dataset(test_tensor,node_batch_merge)"
      ],
      "metadata": {
        "id": "bLMSSN2c92Gq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph_spec  = train_node_dataset.element_spec[0]\n",
        "input_graph = tf.keras.layers.Input(type_spec=graph_spec)"
      ],
      "metadata": {
        "id": "wo8w0AAX_Cj3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_mapping_neurons = 0\n",
        "def set_initial_node_state(node_set, node_set_name):\n",
        "    if n_mapping_neurons == 0:\n",
        "        features = node_set['node_feature']  # works (no mapping)\n",
        "    else:\n",
        "        features = tf.keras.layers.Dense(n_mapping_neurons,activation=\"relu\")(node_set['node_feature']) # works (1 layer mapping)\n",
        "        # features = tf.keras.layers.Dense(8,activation=\"relu\")(tf.keras.layers.Dense(8,activation=\"relu\")(node_set['node_feature'])) # works (2 layer mapping)\n",
        "    return features\n",
        "    # Or we can also have multiple features\n",
        "    # original_features = node_set['x_norm']\n",
        "    # embeded_features  = tf.keras.layers.Dense(8,activation=\"relu\")(node_set['x_norm'])\n",
        "    # return tf.keras.layers.Concatenate()([original_features, embeded_features])\n",
        "\n",
        "\n",
        "\n",
        "def set_initial_edge_state(edge_set, edge_set_name):\n",
        "    if n_mapping_neurons == 0:\n",
        "        features = edge_set['edge_feature'] # no mapping\n",
        "    else:\n",
        "        features = tf.keras.layers.Dense(8,activation=\"relu\")(edge_set['edge_feature'])\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "graph = tfgnn.keras.layers.MapFeatures(\n",
        "    node_sets_fn=set_initial_node_state,\n",
        "    edge_sets_fn=set_initial_edge_state\n",
        ")(input_graph)  # KerasTensor\n"
      ],
      "metadata": {
        "id": "vuvmsqBO95GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dense_layer(units=1, l2_reg=0.1, activation='relu'):\n",
        "    regularizer = tf.keras.regularizers.l2(l2_reg)\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units,\n",
        "                              kernel_regularizer=regularizer,\n",
        "                              bias_regularizer=regularizer,\n",
        "                              activation=activation)])\n",
        "\n",
        "\n",
        "def n_dense_layer(n_layers=1, units=8,l2_reg=0.01, dropout=0, activation='relu'):\n",
        "    regularizer = tf.keras.regularizers.l2(l2_reg)\n",
        "    model = tf.keras.Sequential()\n",
        "    for i in range(n_layers):\n",
        "        model.add(layers.Dense(units, activation=activation, kernel_regularizer=regularizer ))\n",
        "\n",
        "        if dropout>0:\n",
        "            model.add(layers.Dropout(dropout))\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "YnNv1lVt_DbY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_layers            = 1\n",
        "l2_reg              = 1e-8\n",
        "dropout             = 0 # between 0 and 1\n",
        "activation          = 'relu'\n",
        "n_neurons_per_layer = 1\n",
        "n_concat_neurons    = 1\n",
        "\n",
        "\n",
        "graph_updates = 1\n",
        "for i in range(graph_updates):\n",
        "    graph = tfgnn.keras.layers.GraphUpdate(\n",
        "        # edge update\n",
        "        # for each edge, concatenate Target and Source node features\n",
        "        # then pass it through a dense layer, which has n_concat_neurons output\n",
        "        #\n",
        "        # node update\n",
        "        # for each node, if connected to multiple edges, the results of each updated edge features are pooled (summed)\n",
        "        # then concatenated with the current node feature and the results is pass through another concat layer\n",
        "        #\n",
        "        edge_sets = {\n",
        "                    'edge_set1':\n",
        "                    tfgnn.keras.layers.EdgeSetUpdate(\n",
        "                                            next_state=tfgnn.keras.layers.NextStateFromConcat(tf.keras.layers.Dense(n_concat_neurons)),\n",
        "                                            edge_input_feature      = tfgnn.HIDDEN_STATE,\n",
        "                                            node_input_tags         = (tfgnn.SOURCE, tfgnn.TARGET),\n",
        "                                            node_input_feature      = tfgnn.HIDDEN_STATE,\n",
        "                                            context_input_feature   = None      ),\n",
        "\n",
        "\n",
        "                    'edge_set2':\n",
        "                    tfgnn.keras.layers.EdgeSetUpdate(\n",
        "                                            next_state=tfgnn.keras.layers.NextStateFromConcat(tf.keras.layers.Dense(n_concat_neurons)),\n",
        "                                            edge_input_feature      = tfgnn.HIDDEN_STATE,\n",
        "                                            node_input_tags         = (tfgnn.SOURCE, tfgnn.TARGET),\n",
        "                                            node_input_feature      = tfgnn.HIDDEN_STATE,\n",
        "                                            context_input_feature   = None      )\n",
        "                    },\n",
        "\n",
        "       node_sets={\n",
        "                   \"node_set1\":\n",
        "                  tfgnn.keras.layers.NodeSetUpdate(\n",
        "                        {\"edge_set1\": tfgnn.keras.layers.Pool(tfgnn.SOURCE, \"sum\") },\n",
        "                        tfgnn.keras.layers.NextStateFromConcat(tf.keras.layers.Dense(n_concat_neurons))),\n",
        "\n",
        "                   \"node_set2\":\n",
        "                  tfgnn.keras.layers.NodeSetUpdate(\n",
        "                        {\"edge_set2\": tfgnn.keras.layers.Pool(tfgnn.SOURCE, \"sum\") },\n",
        "                        tfgnn.keras.layers.NextStateFromConcat(tf.keras.layers.Dense(n_concat_neurons)))\n",
        "\n",
        "                  })(graph) #start here\n",
        "\n",
        "    # output = tf.keras.layers.Dense(1,activation='linear')(graph.node_sets[\"node_set1\"][tfgnn.HIDDEN_STATE])\n",
        "    # output = tf.keras.layers.Dense(1,activation='linear')(graph.node_sets[\"node_set2\"][tfgnn.HIDDEN_STATE])\n",
        "\n",
        "    output = tf.keras.layers.Dense(1,activation='linear')\\\n",
        "                ( tf.concat(\n",
        "                              [graph.node_sets[\"node_set1\"][tfgnn.HIDDEN_STATE] ,\n",
        "                              graph.node_sets[\"node_set2\"][tfgnn.HIDDEN_STATE] ], axis=0 ))\n",
        "\n",
        "\n",
        "node_model = tf.keras.Model(input_graph, output)\n",
        "\n",
        "\n",
        "##### compile\n",
        "learning_rate = 0.02\n",
        "node_model.compile(\n",
        "    tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "    loss = tf.keras.losses.MeanSquaredError()\n",
        ")\n",
        "\n",
        "node_model.summary()\n",
        "\n",
        "\n",
        "es = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',mode='min',verbose=1,\n",
        "        patience=10,restore_best_weights=True)\n",
        "\n",
        "\n",
        "history = node_model.fit(train_node_dataset.repeat(),\n",
        "                           validation_data=val_node_dataset,\n",
        "                           steps_per_epoch=10,\n",
        "                           epochs=125,\n",
        "                           callbacks=[es])\n",
        "\n",
        "\n",
        "loss = node_model.evaluate(train_node_dataset)\n",
        "loss = node_model.evaluate(val_node_dataset)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_loss(history):\n",
        "  plt.plot(history.history['loss'], label='loss')\n",
        "  plt.plot(history.history['val_loss'], label='val_loss')\n",
        "  # plt.ylim([0, 10])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Error')\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "plot_loss(history)\n",
        "\n",
        "# True Vs Prediction plot\n",
        "y    = np.array(node_df_train['label'])\n",
        "yhat = node_model.predict(train_node_dataset).flatten()\n",
        "Err  = abs((y-yhat))\n",
        "\n",
        "y_val    = np.array(node_df_val['label'])\n",
        "yhat_val = node_model.predict(val_node_dataset).flatten()\n",
        "Err      = abs((y_val-yhat_val))\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(go.Scatter( x=[-15,15], y=[-15,15], mode = 'lines', line=dict(color='black')))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=y.flatten(),\n",
        "                         y=yhat.flatten(),\n",
        "                         mode = 'markers',\n",
        "                         line    = dict(color='blue'),\n",
        "                         marker  = dict(color='blue'),\n",
        "                         name= 'label ',\n",
        "                         ))\n",
        "\n",
        "fig.add_trace(go.Scatter(x=y_val.flatten(),\n",
        "                         y=yhat_val.flatten(),\n",
        "                         mode = 'markers',\n",
        "                         line    = dict(color='red'),\n",
        "                         marker  = dict(color='red'),\n",
        "                         name= 'label ',\n",
        "                         ))\n",
        "fig.update_xaxes( title_text=\"True label\")\n",
        "fig.update_yaxes( title_text=\"Predicted label\")"
      ],
      "metadata": {
        "id": "VhRX0wAU_Ktg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "node_model.get_weights()\n",
        "\n",
        "weights = node_model.weights\n",
        "for weight in weights:\n",
        "    weight_values = weight.numpy().flatten()\n",
        "    layer_name    = weight.name.split('/')[0]  # Extract the layer name from the weight name\n",
        "    print(f\"Weight name: {weight.name}, Layer name: {layer_name}, Weight Values: {weight_values}\")"
      ],
      "metadata": {
        "id": "RquP4c-n_cA0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}